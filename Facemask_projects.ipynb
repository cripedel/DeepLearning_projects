{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Facemask_projects.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cripedel/DeepLearning_projects/blob/DeepLearning_projects/Facemask_projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuStqV2puUNe"
      },
      "source": [
        "# First imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxfidnhpuUNf"
      },
      "source": [
        "## Initial global imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3eFY0wSuaEa",
        "outputId": "b27f3fef-ec6a-4c80-f4c1-b2bf19a2010c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39rCpyehua2X",
        "outputId": "b1076ba0-66f0-48bb-973a-a0189dad0db4"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Dataset.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Dataset.zip\n",
            "replace MaskDataset/test/10001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw4vYqSOKx_e",
        "outputId": "15f35548-cce6-4379-e48a-f70d19b80887"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uruWltGruUNg"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neG5uj7HuUNh"
      },
      "source": [
        "## Necessary to work with JSON and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasMrkmZuUNh"
      },
      "source": [
        "import json\n",
        "import shutil\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdrjUFpIuUNh"
      },
      "source": [
        "# Preprocessing the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnSxEGNuUNh"
      },
      "source": [
        "## Initial directories to work with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7lZzg24uUNi"
      },
      "source": [
        "# Defining the datasets directory\n",
        "dataset_dir = os.path.join(cwd, 'MaskDataset')\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "validation_dir = os.path.join(dataset_dir, 'validation')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "# Create validation directory if it doesn't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIFXIYiguUNi"
      },
      "source": [
        "## Defining images classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lGT6GaVuUNi"
      },
      "source": [
        "# Loading the classes of each image into the memory\n",
        "train_classes_json_file_name = 'train_gt.json'\n",
        "train_classes_json_directory = os.path.join(dataset_dir, train_classes_json_file_name)\n",
        "\n",
        "data = {}\n",
        "\n",
        "with open(train_classes_json_directory) as json_file:\n",
        "    data = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_K5oDsHuUNj"
      },
      "source": [
        "## Preparing folders for Keras usage in a later moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPukL3uAuUNj"
      },
      "source": [
        "# Creating folder for each class of image for training and validation datasets\n",
        "classes = set(data.values())\n",
        "\n",
        "for class_label in classes:\n",
        "    class_training_dir = os.path.join(training_dir, str(class_label))\n",
        "    class_validation_dir = os.path.join(validation_dir, str(class_label))\n",
        "    if not os.path.exists(class_training_dir):\n",
        "        os.makedirs(class_training_dir)\n",
        "    if not os.path.exists(class_validation_dir):\n",
        "        os.makedirs(class_validation_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_qDJksNuUNj"
      },
      "source": [
        "# Assigning images to each training folder/class, avoiding to have the same image two times in the same folder\n",
        "for entry in os.scandir(training_dir):\n",
        "    if(entry.is_file()):\n",
        "        file_destination = os.path.join(training_dir, str(data[entry.name]), entry.name)\n",
        "        if not os.path.isfile(file_destination):\n",
        "            shutil.copy(entry.path, file_destination)\n",
        "    \n",
        "# Choosing random images to be into the validation folders, being able to repeat without cloning images\n",
        "validation_rate = 0.1\n",
        "\n",
        "for class_label in classes:\n",
        "    class_training_dir = os.path.join(training_dir, str(class_label))\n",
        "    class_validation_dir = os.path.join(validation_dir, str(class_label))\n",
        "    \n",
        "    for old_entry in os.scandir(class_validation_dir):\n",
        "        os.remove(old_entry.path)\n",
        "    \n",
        "    training_entries = list(os.scandir(class_training_dir))\n",
        "    validation_size = round(len(training_entries)*validation_rate)\n",
        "    \n",
        "    for validation_entry in random.sample(training_entries, validation_size):\n",
        "        destination = os.path.join(class_validation_dir, validation_entry.name)\n",
        "        os.rename(validation_entry.path, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw_Gx2FouUNj"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZhdMlDuUNj"
      },
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB3lfY3qJhGd",
        "outputId": "a1504242-85ca-418f-eb35-c16cf9e160e6"
      },
      "source": [
        "x = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    training_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=8,\n",
        "    image_size=(256, 256),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation=\"bilinear\",\n",
        "    follow_links=False,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5052 files belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "ZmLalCu2NHLP",
        "outputId": "6a65168f-733d-4b47-b669-f150c07eee65"
      },
      "source": [
        "ls MaskDataset/training/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-a2236b282b82>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ls MaskDataset/training/\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "LYBcVxb7KK3R",
        "outputId": "2250b805-0ba8-47f0-e03c-1ca6a564ac3c"
      },
      "source": [
        "from keras.applications import imagenet_utils\n",
        "\n",
        "tf.keras.applications.imagenet_utils.preprocess_input(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-99b0db42b596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagenet_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     return _preprocess_symbolic_input(\n\u001b[0;32m--> 119\u001b[0;31m         x, data_format=data_format, mode=mode)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0;31m# 'RGB'->'BGR'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m103.939\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m116.779\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123.68\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'BatchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1na9o7YuUNj"
      },
      "source": [
        "apply_data_augmentation = True\n",
        "\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=10,\n",
        "        height_shift_range=10,\n",
        "        zoom_range=0.3,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='constant',\n",
        "        cval=0,\n",
        "        rescale=1/255.\n",
        "    )\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "# test_data_gen = ImageDataGenerator(rescale=1/255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_3SlgZuUNk"
      },
      "source": [
        "## Data loader from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of0ud2nguUNk"
      },
      "source": [
        "bs = 8\n",
        "\n",
        "train_gen = train_data_gen.flow_from_directory(\n",
        "    training_dir,\n",
        "    batch_size=bs,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "\n",
        "valid_gen = valid_data_gen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    batch_size=bs,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# test_gen = test_data_gen.flow_from_directory(\n",
        "#     test_dir,\n",
        "#     batch_size=bs,\n",
        "#     class_mode='categorical',\n",
        "#     shuffle=True,\n",
        "#     seed=SEED\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7zgcLMCg6Lf"
      },
      "source": [
        "tf.keras.applications.imagenet_utils.preprocess_input?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjhBMQ9yuUNo"
      },
      "source": [
        "\n",
        "from keras.applications import imagenet_utils\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "num_classes = len(classes)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_gen,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 256, 256, 3], [None, num_classes])\n",
        ")\n",
        "\n",
        "type(train_dataset)\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "tf.keras.applications.imagenet_utils.preprocess_input(\n",
        "    train_gen, data_format=None, mode='caffe'\n",
        ")\n",
        "\n",
        "type(train_dataset)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: valid_gen,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 32, 32, 3], [None, num_classes])\n",
        ")\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "\n",
        "# test_dataset = tf.data.Dataset.from_generator(\n",
        "#     lambda: test_gen,\n",
        "#     output_types=(tf.float32, tf.float32),\n",
        "#     output_shapes=([None, 256, 256, 3], [None, num_classes])\n",
        "# )\n",
        "\n",
        "# test_dataset = test_dataset.repeat()\n",
        "type(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPJ_ucw8uUNp"
      },
      "source": [
        "## Testing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUE6uFquUNp"
      },
      "source": [
        "iterator = iter(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8hpJ3tluUNp"
      },
      "source": [
        "augmented_img, target = next(iterator)\n",
        "\n",
        "augmented_img = np.array(augmented_img[0]) # it was a tensor, so we need to transform it into an array\n",
        "augmented_img = augmented_img * 255 # normalizing\n",
        "\n",
        "plt.imshow(np.uint8(augmented_img))\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMffrQguUNq"
      },
      "source": [
        "# CNN part ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmo-T6UHuUNq"
      },
      "source": [
        "## Building the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3O7IN3uUNq"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import imagenet_utils\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.engine import Model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from keras.layers import Dropout\n",
        "input_shape = (256, 256, 3)\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "for layer in base_model.layers: \n",
        "  #layer.trainable = False\n",
        "  #print('Capa ' + layer.name + ' congelada...')\n",
        "  last = base_model.layers[-1].output\n",
        "x = Flatten()(last)\n",
        "x = Dense(1000, activation='relu', name='fc1')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(200, activation='relu', name='fc2')(x)\n",
        "x = Dense(3, activation='softmax', name='predictions')(x)\n",
        "model = Model(base_model.input, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPtMIY-LuUNq"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsPuuon8uUNq"
      },
      "source": [
        "## Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki761pskuUNq"
      },
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "lr = 1e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQajwkDHQfP"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/My\\ Drive/Keras3/classification_experiments/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzpRKbkJ-bea"
      },
      "source": [
        "import os   #El codigo es siempre igual ? \n",
        "from datetime import datetime\n",
        "#Start f ; caracteristicas iniciales \n",
        "cwd = os.getcwd()\n",
        "\n",
        "exps_dir = os.path.join('/content/drive/My Drive/Keras3/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "# ----------------\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixghz28fuUNr"
      },
      "source": [
        "model.fit(\n",
        "    x=train_dataset,\n",
        "    epochs=40,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_data=valid_dataset,\n",
        "    validation_steps=len(valid_gen),\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV-4_DQBuUNr"
      },
      "source": [
        "exps_dir = os.path.join(cwd, 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "# ----------------\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(ckpt_dir, 'cp_.ckpt'), #'cp_{epoch:02d}.ckpt'\n",
        "    save_weights_only=True # False to save the model directly\n",
        ")\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "\n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=tb_dir,\n",
        "    profile_batch=0,\n",
        "    histogram_freq=1 # if 1 shows weights histograms\n",
        ")\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6VDqaZuUNq"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'D:\\Polimi\\Lectures\\3semester\\artificial-neural-networks-and-deep-learning\\Homeworks\\1st\\classification_experiments\\'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}